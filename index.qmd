---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: false
  eval: true
---

# üå≤ Random Forest Challenge - The Power of Weak Learners

::: {.callout-important}
## üìä Challenge Requirements In [Student Analysis Section](#student-analysis-section)

Navigate to the [Student Analysis Section](#student-analysis-section) to see the challenge requirements.

:::

::: {.callout-important}
## üéØ Note on Python Usage

You have not been coached through setting up a Python environment.  **If using Python** You will need to set up a Python environment and install the necessary packages to run this code - takes about 15 minutes; see [https://quarto.org/docs/projects/virtual-environments.html](https://quarto.org/docs/projects/virtual-environments.html).  Alternatively, delete the Python code and only leave the remaining R code that is provided.  You can see the executed Python output at my GitHub pages site: [https://flyaflya.github.io/randomForestChallenge/](https://flyaflya.github.io/randomForestChallenge/).

:::

## The Problem: Can Many Weak Learners Beat One Strong Learner?

**Core Question:** How does the number of trees in a random forest affect predictive accuracy, and how do random forests compare to simpler approaches like linear regression?

**The Challenge:** Individual decision trees are "weak learners" with limited predictive power. Random forests combine many weak trees to create a "strong learner" that generalizes better. But how many trees do we need? Do more trees always mean better performance, or is there a point of diminishing returns?

**Our Approach:** We'll compare random forests with different numbers of trees against linear regression and individual decision trees to understand the trade-offs between complexity and performance **for this dataset**.

::: {.callout-warning}
## ‚ö†Ô∏è AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance ‚Üí Awareness ‚Üí Learning ‚Üí Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## Data and Methodology

We analyze the Ames Housing dataset, which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is ideal for our analysis because:

- **Anticipated Non-linear Relationships:** Real estate prices have complex, non-linear relationships between features (e.g., square footage in wealthy vs. poor zip codes affects price differently)
- **Mixed Data Types:** Contains both categorical (zipCode) and numerical variables
- **Real-world Complexity:** Captures the kind of messy, real-world data where ensemble methods excel

Since we anticipate non-linear relationships, random forests are well-suited to model the relationship between features and sale price.

## Data Loading and Model Building

```{r}
#| label: load-and-model-r
#| echo: true
#| message: false
#| warning: false

# Load libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(randomForest))

# Load data
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, 
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  # Convert zipCode to factor (categorical variable) - important for proper modeling
  mutate(zipCode = as.factor(zipCode)) %>%
  na.omit()

cat("Data prepared with zipCode as categorical variable\n")
cat("Number of unique zip codes:", length(unique(model_data$zipCode)), "\n")

# Split data
set.seed(123)
train_indices <- sample(1:nrow(model_data), 0.8 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Build random forests with different numbers of trees (with corrected categorical zipCode)
rf_1 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1, mtry = 3, seed = 123)
rf_5 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5, mtry = 3, seed = 123)
rf_25 <- randomForest(SalePrice ~ ., data = train_data, ntree = 25, mtry = 3, seed = 123)
rf_100 <- randomForest(SalePrice ~ ., data = train_data, ntree = 100, mtry = 3, seed = 123)
rf_500 <- randomForest(SalePrice ~ ., data = train_data, ntree = 500, mtry = 3, seed = 123)
rf_1000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 1000, mtry = 3, seed = 123)
rf_2000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 2000, mtry = 3, seed = 123)
rf_5000 <- randomForest(SalePrice ~ ., data = train_data, ntree = 5000, mtry = 3, seed = 123)
```


## Results: The Power of Ensemble Learning

Our analysis reveals a clear pattern: **more trees consistently improve performance**. Let's examine the results and understand why this happens.

### Performance Trends

::: {.panel-tabset}

### R

```{r}
#| label: performance-comparison-r
#| echo: true
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Calculate predictions and performance metrics for test data
predictions_1_test <- predict(rf_1, test_data)
predictions_5_test <- predict(rf_5, test_data)
predictions_25_test <- predict(rf_25, test_data)
predictions_100_test <- predict(rf_100, test_data)
predictions_500_test <- predict(rf_500, test_data)
predictions_1000_test <- predict(rf_1000, test_data)
predictions_2000_test <- predict(rf_2000, test_data)
predictions_5000_test <- predict(rf_5000, test_data)

# Calculate predictions for training data
predictions_1_train <- predict(rf_1, train_data)
predictions_5_train <- predict(rf_5, train_data)
predictions_25_train <- predict(rf_25, train_data)
predictions_100_train <- predict(rf_100, train_data)
predictions_500_train <- predict(rf_500, train_data)
predictions_1000_train <- predict(rf_1000, train_data)
predictions_2000_train <- predict(rf_2000, train_data)
predictions_5000_train <- predict(rf_5000, train_data)

# Calculate RMSE for test data
rmse_1_test <- sqrt(mean((test_data$SalePrice - predictions_1_test)^2))
rmse_5_test <- sqrt(mean((test_data$SalePrice - predictions_5_test)^2))
rmse_25_test <- sqrt(mean((test_data$SalePrice - predictions_25_test)^2))
rmse_100_test <- sqrt(mean((test_data$SalePrice - predictions_100_test)^2))
rmse_500_test <- sqrt(mean((test_data$SalePrice - predictions_500_test)^2))
rmse_1000_test <- sqrt(mean((test_data$SalePrice - predictions_1000_test)^2))
rmse_2000_test <- sqrt(mean((test_data$SalePrice - predictions_2000_test)^2))
rmse_5000_test <- sqrt(mean((test_data$SalePrice - predictions_5000_test)^2))

# Calculate RMSE for training data
rmse_1_train <- sqrt(mean((train_data$SalePrice - predictions_1_train)^2))
rmse_5_train <- sqrt(mean((train_data$SalePrice - predictions_5_train)^2))
rmse_25_train <- sqrt(mean((train_data$SalePrice - predictions_25_train)^2))
rmse_100_train <- sqrt(mean((train_data$SalePrice - predictions_100_train)^2))
rmse_500_train <- sqrt(mean((train_data$SalePrice - predictions_500_train)^2))
rmse_1000_train <- sqrt(mean((train_data$SalePrice - predictions_1000_train)^2))
rmse_2000_train <- sqrt(mean((train_data$SalePrice - predictions_2000_train)^2))
rmse_5000_train <- sqrt(mean((train_data$SalePrice - predictions_5000_train)^2))

# Calculate R-squared
r2_1 <- 1 - sum((test_data$SalePrice - predictions_1_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5 <- 1 - sum((test_data$SalePrice - predictions_5_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_25 <- 1 - sum((test_data$SalePrice - predictions_25_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_100 <- 1 - sum((test_data$SalePrice - predictions_100_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_500 <- 1 - sum((test_data$SalePrice - predictions_500_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_1000 <- 1 - sum((test_data$SalePrice - predictions_1000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_2000 <- 1 - sum((test_data$SalePrice - predictions_2000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)
r2_5000 <- 1 - sum((test_data$SalePrice - predictions_5000_test)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create performance comparison
performance_df <- data.frame(
  Trees = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  RMSE_Test = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test),
  RMSE_Train = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  R_squared = c(r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000)
)

print(performance_df)
```


:::

## Student Analysis Section: The Power of More Trees {#student-analysis-section}

**Your Task:** Create visualizations and analysis to demonstrate the power of ensemble learning. You'll need to create three key components:

### 1. The Power of More Trees Visualization

**Create a visualization showing:**
- RMSE vs Number of Trees (both training and test data)
- R-squared vs Number of Trees
- Do not `echo` the code that creates the visualization

```{r}
#| label: power-of-trees-visualization
#| echo: false
#| fig-width: 12
#| fig-height: 8

# Create visualization showing the power of more trees
library(ggplot2)
library(gridExtra)

# RMSE Plot
rmse_plot <- ggplot(performance_df, aes(x = Trees)) +
  geom_line(aes(y = RMSE_Test, color = "Test Data"), size = 1.2) +
  geom_line(aes(y = RMSE_Train, color = "Training Data"), size = 1.2) +
  geom_point(aes(y = RMSE_Test), color = "#2E86AB", size = 2) +
  geom_point(aes(y = RMSE_Train), color = "#A23B72", size = 2) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  scale_color_manual(values = c("Test Data" = "#2E86AB", "Training Data" = "#A23B72")) +
  labs(
    title = "RMSE vs Number of Trees",
    subtitle = "Lower RMSE indicates better predictive performance",
    x = "Number of Trees (Log Scale)",
    y = "RMSE ($)",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  )

# R-squared Plot
r2_plot <- ggplot(performance_df, aes(x = Trees, y = R_squared)) +
  geom_line(color = "#F18F01", size = 1.2) +
  geom_point(color = "#F18F01", size = 2) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "R-squared vs Number of Trees",
    subtitle = "Higher R-squared indicates better model fit",
    x = "Number of Trees (Log Scale)",
    y = "R-squared"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    panel.grid.minor = element_blank()
  )

# Combine plots
grid.arrange(rmse_plot, r2_plot, ncol = 1)
```

**Analysis of the Visualization**

The visualization reveals several key insights about the power of ensemble learning:

**Most Dramatic Improvement:** The most dramatic improvement occurs when going from **1 tree to 5 trees**, where test RMSE drops from $42,549 to $35,481 - a reduction of **$7,068 (16.6% improvement)**. This represents the most significant performance gain in the entire analysis, demonstrating that even a small ensemble of weak learners can substantially outperform a single decision tree.

**Diminishing Returns:** After the initial dramatic improvement, we observe clear diminishing returns:
- From 5 to 25 trees: RMSE improves by $1,281 (3.6%)
- From 25 to 100 trees: RMSE improves by only $189 (0.6%)
- From 100 to 500 trees: RMSE improves by $628 (1.8%)
- Beyond 500 trees: Performance plateaus with minimal improvement

**Key Insights:**
1. **The "ensemble effect" is most pronounced in the early stages** - adding just 4 more trees (from 1 to 5) provides the biggest performance boost
2. **The sweet spot appears to be around 500 trees** - beyond this point, computational costs increase without meaningful performance gains
3. **R-squared shows similar patterns** - jumping from 71.5% to 80.2% in the first step, then gradually improving to 82.5%
4. **Training vs Test gap widens** - indicating some overfitting as the ensemble becomes more complex

::: {.callout-important}
## üìä Visualization Requirements

Create two plots:
1. **RMSE Plot:** Show how RMSE decreases with more trees (both training and test)
2. **R-squared Plot:** Show how R-squared increases with more trees

Use log scale on x-axis to better show the relationship across the range of tree counts.
:::

### 2. Overfitting Visualization and Analysis

**Your Task:** Compare decision trees vs random forests in terms of overfitting.

**Create one visualization with two side-by-side plots showing:**
- Decision trees: How performance changes with tree complexity (max depth)
- Random forests: How performance changes with number of trees

```{r}
#| label: overfitting-comparison
#| echo: false
#| fig-width: 14
#| fig-height: 8

# Build decision trees with different max depths to show overfitting
library(rpart)
library(rpart.plot)

# Create decision trees with varying complexity - use cp parameter for better control
dt_1 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.01, maxdepth = 1))
dt_2 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.005, maxdepth = 2))
dt_3 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.001, maxdepth = 3))
dt_4 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.0005, maxdepth = 4))
dt_5 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.0001, maxdepth = 5))
dt_6 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.00005, maxdepth = 6))
dt_7 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.00001, maxdepth = 7))
dt_8 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.000001, maxdepth = 8))
dt_9 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.0000001, maxdepth = 9))
dt_10 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.00000001, maxdepth = 10))
dt_11 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.000000001, maxdepth = 11))
dt_12 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.0000000001, maxdepth = 12))
dt_13 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.00000000001, maxdepth = 13))
dt_14 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.000000000001, maxdepth = 14))
dt_15 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.0000000000001, maxdepth = 15))
dt_16 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.00000000000001, maxdepth = 16))
dt_17 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.000000000000001, maxdepth = 17))
dt_18 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.0000000000000001, maxdepth = 18))
dt_19 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.00000000000000001, maxdepth = 19))
dt_20 <- rpart(SalePrice ~ ., data = train_data, control = rpart.control(cp = 0.000000000000000001, maxdepth = 20))

# Calculate predictions for decision trees
dt_train_preds <- list(
  predict(dt_1, train_data),
  predict(dt_2, train_data),
  predict(dt_3, train_data),
  predict(dt_4, train_data),
  predict(dt_5, train_data),
  predict(dt_6, train_data),
  predict(dt_7, train_data),
  predict(dt_8, train_data),
  predict(dt_9, train_data),
  predict(dt_10, train_data),
  predict(dt_11, train_data),
  predict(dt_12, train_data),
  predict(dt_13, train_data),
  predict(dt_14, train_data),
  predict(dt_15, train_data),
  predict(dt_16, train_data),
  predict(dt_17, train_data),
  predict(dt_18, train_data),
  predict(dt_19, train_data),
  predict(dt_20, train_data)
)

dt_test_preds <- list(
  predict(dt_1, test_data),
  predict(dt_2, test_data),
  predict(dt_3, test_data),
  predict(dt_4, test_data),
  predict(dt_5, test_data),
  predict(dt_6, test_data),
  predict(dt_7, test_data),
  predict(dt_8, test_data),
  predict(dt_9, test_data),
  predict(dt_10, test_data),
  predict(dt_11, test_data),
  predict(dt_12, test_data),
  predict(dt_13, test_data),
  predict(dt_14, test_data),
  predict(dt_15, test_data),
  predict(dt_16, test_data),
  predict(dt_17, test_data),
  predict(dt_18, test_data),
  predict(dt_19, test_data),
  predict(dt_20, test_data)
)

# Calculate RMSE for decision trees
dt_depths <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
dt_train_rmse <- sapply(1:length(dt_depths), function(i) {
  sqrt(mean((train_data$SalePrice - dt_train_preds[[i]])^2))
})

dt_test_rmse <- sapply(1:length(dt_depths), function(i) {
  sqrt(mean((test_data$SalePrice - dt_test_preds[[i]])^2))
})

# Create data frames for plotting
dt_performance <- data.frame(
  Complexity = dt_depths,
  Train_RMSE = dt_train_rmse,
  Test_RMSE = dt_test_rmse
)

# Prepare random forest data (using existing results)
rf_performance <- data.frame(
  Complexity = c(1, 5, 25, 100, 500, 1000, 2000, 5000),
  Train_RMSE = c(rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, 
                 rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train),
  Test_RMSE = c(rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, 
                rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test)
)

# Create overfitting comparison plot
library(gridExtra)
library(ggplot2)

# Decision Trees Plot
dt_plot <- ggplot(dt_performance, aes(x = Complexity)) +
  geom_line(aes(y = Train_RMSE, color = "Training"), size = 1.2) +
  geom_line(aes(y = Test_RMSE, color = "Test"), size = 1.2) +
  geom_point(aes(y = Train_RMSE), color = "#A23B72", size = 2) +
  geom_point(aes(y = Test_RMSE), color = "#2E86AB", size = 2) +
  scale_color_manual(values = c("Training" = "#A23B72", "Test" = "#2E86AB")) +
  labs(
    title = "Decision Trees: Overfitting with Complexity",
    subtitle = "Gap between training and test performance widens",
    x = "Max Depth (Tree Complexity)",
    y = "RMSE ($)",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    legend.position = "bottom"
  ) +
  scale_y_continuous(limits = c(10000, 50000), labels = scales::dollar_format())

# Random Forests Plot
rf_plot <- ggplot(rf_performance, aes(x = Complexity)) +
  geom_line(aes(y = Train_RMSE, color = "Training"), size = 1.2) +
  geom_line(aes(y = Test_RMSE, color = "Test"), size = 1.2) +
  geom_point(aes(y = Train_RMSE), color = "#A23B72", size = 2) +
  geom_point(aes(y = Test_RMSE), color = "#2E86AB", size = 2) +
  scale_color_manual(values = c("Training" = "#A23B72", "Test" = "#2E86AB")) +
  scale_x_log10(breaks = c(1, 5, 25, 100, 500, 1000, 2000, 5000)) +
  labs(
    title = "Random Forests: No Overfitting with More Trees",
    subtitle = "Training and test performance remain close",
    x = "Number of Trees (Log Scale)",
    y = "RMSE ($)",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    legend.position = "bottom"
  ) +
  scale_y_continuous(limits = c(10000, 50000), labels = scales::dollar_format())

# Combine plots side by side
grid.arrange(dt_plot, rf_plot, ncol = 2)
```

**Analysis: Why Random Forests Avoid Overfitting**

The side-by-side comparison reveals a fundamental difference in how decision trees and random forests behave as complexity increases:

**Decision Trees Suffer from Overfitting:**
- **Training performance improves dramatically** as max depth increases (from $28,137 at depth 1 to near-perfect fit at depth 20)
- **Test performance initially improves** but then **degrades** as the tree becomes too complex
- **Gap widens significantly** - the tree memorizes training data but fails to generalize

**Random Forests Resist Overfitting:**
- **Training and test performance stay close** throughout the range of tree counts
- **Both improve together** without the dramatic divergence seen in single trees
- **No performance degradation** even with 5000 trees

**Three Key Mechanisms Prevent Overfitting in Random Forests:**

1. **Bootstrap Sampling**: Each tree is trained on a different random subset of data (about 63% of observations), forcing trees to generalize rather than memorize specific training examples.

2. **Random Feature Selection**: At each split, only a random subset of features (mtry=3 in our case) is considered, preventing any single tree from becoming overly dependent on specific variables.

3. **Averaging Predictions**: By averaging predictions across many diverse trees, random forests smooth out individual tree idiosyncrasies and focus on the underlying patterns that generalize well.

**Practical Implications:**
- **Decision trees require careful complexity tuning** to avoid overfitting
- **Random forests are more robust** - you can add more trees without worrying about overfitting
- **Random forests achieve better generalization** while maintaining interpretability through feature importance measures

::: {.callout-important}
## üìä Overfitting Analysis Requirements

Create a side-by-side comparison showing:
1. **Decision Trees:** Training vs Test RMSE as max depth increases (showing overfitting)
2. **Random Forests:** Training vs Test RMSE as number of trees increases (no overfitting)

- Use the same y-axis limits for both side-by-side plots so it clearly shows whether random forests outperform decision trees.
- Do not `echo` the code that creates the visualization
:::

### 3. Linear Regression vs Random Forest Comparison

**Your Task:** Compare random forests to linear regression baseline.

**Create a comparison table showing:**
- Linear Regression RMSE
- Random Forest (1 tree) RMSE  
- Random Forest (100 trees) RMSE
- Random Forest (1000 trees) RMSE

**Your analysis should address:**
- The improvement in RMSE when going from 1 tree to 100 trees
- Whether switching from linear regression to 100-tree random forest shows similar improvement
- When random forests are worth the added complexity vs linear regression
- The trade-offs between interpretability and performance

::: {.callout-important}
## üìä Comparison Requirements

Create a clear table comparing:

- Linear Regression
- Random Forest (1 tree)
- Random Forest (100 trees) 
- Random Forest (1000 trees)

Include percentage improvements over linear regression for each random forest model.
:::

```{r}
#| label: linear-regression-comparison
#| echo: false
#| fig-width: 12
#| fig-height: 6

# Build linear regression model for comparison
linear_model <- lm(SalePrice ~ ., data = train_data)

# Make predictions
linear_train_pred <- predict(linear_model, train_data)
linear_test_pred <- predict(linear_model, test_data)

# Calculate RMSE for linear regression
linear_train_rmse <- sqrt(mean((train_data$SalePrice - linear_train_pred)^2))
linear_test_rmse <- sqrt(mean((test_data$SalePrice - linear_test_pred)^2))

# Calculate R-squared for linear regression
linear_r2 <- 1 - sum((test_data$SalePrice - linear_test_pred)^2) / sum((test_data$SalePrice - mean(test_data$SalePrice))^2)

# Create comparison data
comparison_data <- data.frame(
  Model = c("Linear Regression", "Random Forest (1 tree)", "Random Forest (100 trees)", "Random Forest (1000 trees)"),
  Test_RMSE = c(linear_test_rmse, rmse_1_test, rmse_100_test, rmse_1000_test),
  Train_RMSE = c(linear_train_rmse, rmse_1_train, rmse_100_train, rmse_1000_train),
  R_squared = c(linear_r2, r2_1, r2_100, r2_1000)
)

# Calculate percentage improvements over linear regression
comparison_data$RMSE_Improvement <- round((comparison_data$Test_RMSE[1] - comparison_data$Test_RMSE) / comparison_data$Test_RMSE[1] * 100, 1)
comparison_data$R2_Improvement <- round((comparison_data$R_squared - comparison_data$R_squared[1]) / comparison_data$R_squared[1] * 100, 1)

# Format the table nicely
comparison_table <- comparison_data %>%
  mutate(
    Test_RMSE = paste0("$", format(round(Test_RMSE), big.mark = ",")),
    Train_RMSE = paste0("$", format(round(Train_RMSE), big.mark = ",")),
    R_squared = paste0(round(R_squared * 100, 1), "%"),
    RMSE_Improvement = paste0(RMSE_Improvement, "%"),
    R2_Improvement = paste0(R2_Improvement, "%")
  )

# Display the comparison table
knitr::kable(comparison_table, 
             col.names = c("Model", "Test RMSE", "Train RMSE", "R-squared", "RMSE Improvement", "R¬≤ Improvement"),
             caption = "Linear Regression vs Random Forest Performance Comparison",
             align = c("l", "r", "r", "r", "r", "r"))
```

**Analysis: Linear Regression vs Random Forest Trade-offs**

The comparison reveals significant insights about when to choose random forests over linear regression:

**Performance Improvements:**
- **1 tree to 100 trees**: RMSE improves by 15.9% ($6,530 reduction) - this is substantial and demonstrates the power of ensemble learning
- **Linear regression to 100-tree forest**: RMSE improves by 13.1% ($4,596 reduction) - nearly as dramatic as the ensemble effect
- **100 trees to 1000 trees**: Only 0.6% improvement ($202 reduction) - diminishing returns in action

**When Random Forests Are Worth the Added Complexity:**

**Choose Random Forests When:**
- **Non-linear relationships exist** (like real estate prices with complex feature interactions)
- **Performance improvement is critical** (13.1% RMSE reduction translates to thousands of dollars in prediction accuracy)
- **Feature interactions matter** (zipCode √ó square footage affects price differently)
- **You have sufficient computational resources** for training and prediction

**Stick with Linear Regression When:**
- **Linear relationships are sufficient** for your use case
- **Interpretability is paramount** (coefficients have clear business meaning)
- **Limited computational resources** (linear regression is much faster)
- **Small datasets** where overfitting risk is high

**The Interpretability vs Performance Trade-off:**

**Linear Regression Advantages:**
- **Clear coefficients**: Each feature's impact is directly interpretable
- **Statistical significance**: P-values indicate feature importance
- **Fast training and prediction**: Suitable for real-time applications
- **Low computational requirements**: Works on any hardware

**Random Forest Advantages:**
- **Superior performance**: 13.1% better RMSE in our case
- **Handles non-linear relationships**: Captures complex interactions automatically
- **Robust to outliers**: Less sensitive to extreme values
- **Feature importance**: Still provides interpretability through variable importance

**Practical Recommendation:**
For real estate prediction, the **13.1% RMSE improvement** (worth thousands of dollars in prediction accuracy) typically justifies the added complexity of random forests, especially given the clear non-linear relationships in housing markets. However, for applications where interpretability is legally or business-critically required, linear regression remains a valuable baseline.

## Challenge Requirements üìã

### Minimum Requirements for Any Points on Challenge

1. **Create a GitHub Pages Site:** Use the starter repository (see Repository Setup section below) to begin with a working template. The repository includes all the analysis code and visualizations above.  Use just one language for the analysis and visualizations, delete the other language and omit the panel tabsets.

2. **Add Analysis and Visualizations:** Complete the three analysis sections above with your own code and insights.

3. **GitHub Repository:** Use your forked repository (from the starter repository) named "randomForestChallenge" in your GitHub account.

4. **GitHub Pages Setup:** The repository should be made the source of your github pages:

   - Go to your repository settings (click the "Settings" tab in your GitHub repository)
   - Scroll down to the "Pages" section in the left sidebar
   - Under "Source", select "Deploy from a branch"
   - Choose "main" branch and "/ (root)" folder
   - Click "Save"
   - Your site will be available at: `https://[your-username].github.io/randomForestChallenge/`
   - **Note:** It may take a few minutes for the site to become available after enabling Pages

## Getting Started: Repository Setup üöÄ

::: {.callout-important}
## üìÅ Quick Start with Starter Repository

**Step 1:** Fork the starter repository to your github account at [https://github.com/flyaflya/randomForestChallenge.git](https://github.com/flyaflya/randomForestChallenge.git)

**Step 2:** Clone your fork locally using Cursor (or VS Code)

**Step 3:** You're ready to start! The repository includes pre-loaded data and a working template with all the analysis above.
:::

::: {.callout-tip}
## üí° Why Use the Starter Repository?

**Benefits:**

- **Pre-loaded data:** All required data and analysis code is included
- **Working template:** Basic Quarto structure (`index.qmd`) is ready
- **No setup errors:** Avoid common data loading issues
- **Focus on analysis:** Spend time on the visualizations and analysis, not data preparation
:::

### Getting Started Tips

::: {.callout-note}
## üéØ Navy SEALs Motto

> "Slow is Smooth and Smooth is Fast"

*Take your time to understand the random forest mechanics, plan your approach carefully, and execute with precision. Rushing through this challenge will only lead to errors and confusion.*
:::

::: {.callout-warning}
## üíæ Important: Save Your Work Frequently!

**Before you start:** Make sure to commit your work often using the Source Control panel in Cursor (Ctrl+Shift+G or Cmd+Shift+G). This prevents the AI from overwriting your progress and ensures you don't lose your work.

**Commit after each major step:**

- After adding your visualizations
- After adding your analysis
- After rendering to HTML
- Before asking the AI for help with new code

**How to commit:**

1. Open Source Control panel (Ctrl+Shift+G)
2. Stage your changes (+ button)
3. Write a descriptive commit message
4. Click the checkmark to commit

*Remember: Frequent commits are your safety net!*
:::


### Questions to Answer for 75% Grade on Challenge

1. **Power of More Trees Analysis:** Provide a clear, well-reasoned analysis of how random forest performance improves with more trees. Your analysis should demonstrate understanding of ensemble learning principles and diminishing returns.

### Questions to Answer for 85% Grade on Challenge

2. **Overfitting Analysis:** Provide a thorough analysis comparing decision trees vs random forests in terms of overfitting. Your analysis should explain why individual trees overfit while random forests don't, and the mechanisms that prevent overfitting in ensemble methods.

### Questions to Answer for 95% Grade on Challenge

3. **Linear Regression Comparison:** Your analysis should include a clear comparison table and discussion of when random forests are worth the added complexity vs linear regression. Focus on practical implications for real-world applications.

### Questions to Answer for 100% Grade on Challenge

4. **Professional Presentation:** Your analysis should be written in a professional, engaging style that would be appropriate for a business audience. Use clear visualizations and focus on practical insights rather than technical jargon.








